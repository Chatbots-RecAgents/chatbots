{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System version: 3.9.6 (default, Sep 26 2022, 11:37:49) \n",
      "[Clang 14.0.0 (clang-1400.0.29.202)]\n",
      "LightGBM version: 3.3.5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "from tempfile import TemporaryDirectory\n",
    "from sklearn.metrics import roc_auc_score, log_loss\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#pip install lightgbm==3.3.5\n",
    "\n",
    "import lightgbm_utils as lgb_utils\n",
    "\n",
    "print(\"System version: {}\".format(sys.version))\n",
    "print(\"LightGBM version: {}\".format(lgb.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "#Defining lgbm parameters\n",
    "MAX_LEAF = 128\n",
    "MIN_DATA = 50\n",
    "NUM_OF_TREES = 100\n",
    "TREE_LEARNING_RATE = 0.1\n",
    "EARLY_STOPPING_ROUNDS = 200\n",
    "METRIC = \"auc\"\n",
    "SIZE = \"sample\"\n",
    "\n",
    "params = {\n",
    "    \"task\": \"train\",\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"num_class\": 1,\n",
    "    \"objective\": \"binary\",\n",
    "    \"metric\": METRIC,\n",
    "    \"num_leaves\": MAX_LEAF,\n",
    "    \"min_data\": MIN_DATA,\n",
    "    \"boost_from_average\": True,\n",
    "    \"num_threads\": 20, \n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"learning_rate\": TREE_LEARNING_RATE,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder() \n",
    "\n",
    "def preprocess_df(path):\n",
    "    df = pd.read_csv(path)\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Dealing with the essay columns\n",
    "    df['all_essays'] = df[['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9']].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "    df = df.drop(columns=['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9'])\n",
    "    df = df.drop(columns=['all_essays'])\n",
    "\n",
    "    # Label encode categorical variables\n",
    "    #label_encoder = LabelEncoder()\n",
    "    for col in df.select_dtypes(include=['object']).columns:\n",
    "        df[col] = label_encoder.fit_transform(df[col])\n",
    "\n",
    "    return df\n",
    "\n",
    "def ratings_prediction(given_profile, df):\n",
    "    # Compute cosine similarity between the given profile and all other profiles\n",
    "    given_profile_df = pd.DataFrame([given_profile])\n",
    "    given_profile_encoded = given_profile_df.copy()\n",
    "    for col in given_profile_encoded.select_dtypes(include=['object']).columns:\n",
    "        given_profile_encoded[col] = label_encoder.fit_transform(given_profile_encoded[col])\n",
    "\n",
    "    # Create a DataFrame with the given profile repeated for each row to match the dimensions of df\n",
    "    given_profile_expanded = pd.concat([given_profile_encoded]*len(df), ignore_index=True)\n",
    "    \n",
    "    # Compute cosine similarity between the given profile and all other profiles\n",
    "    cosine_sim_given = cosine_similarity(given_profile_expanded, df)\n",
    "    df['rating'] = cosine_sim_given.mean(axis=0)\n",
    "\n",
    "    # Normalize the ratings to be between 0 and 1\n",
    "    df['rating'] = (df['rating'] - df['rating'].min()) / (df['rating'].max() - df['rating'].min())\n",
    "\n",
    "    return df\n",
    "\n",
    "def preprocess_lgbm(path, df):\n",
    "    df_lgbm = pd.read_csv(path)\n",
    "\n",
    "    # Dealing with the essay columns\n",
    "    df_lgbm['all_essays'] = df_lgbm[['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9']].apply(lambda x: ' '.join(x.dropna()), axis=1)\n",
    "    df_lgbm = df_lgbm.drop(columns=['essay0', 'essay1', 'essay2', 'essay3', 'essay4', 'essay5', 'essay6', 'essay7', 'essay8', 'essay9'])\n",
    "    df_lgbm = df_lgbm.drop(columns=['all_essays'])\n",
    "\n",
    "    #Adding the rating column gathered through Feature Engineering\n",
    "    merged_df = df_lgbm.merge(df[['rating']], how='left', left_index=True, right_index=True)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def encode_csv(df, encoder, label_col, typ=\"fit\"):\n",
    "    if typ == \"fit\":\n",
    "        df = encoder.fit_transform(df)\n",
    "    else:\n",
    "        df = encoder.transform(df)\n",
    "    y = df[label_col].values\n",
    "    del df[label_col]\n",
    "    return df, y\n",
    "\n",
    "def training_lgbm(merged_df):\n",
    "    #defining the columns from our df\n",
    "    nume_cols = [\"age\", \"height\", \"income\"]\n",
    "    cate_cols = [\"body_type\", \"diet\", \"drinks\", \"drugs\", \"education\", \"ethnicity\", \"job\", \"last_online\",\n",
    "                \"location\", \"offspring\", \"orientation\", \"pets\", \"religion\", \"sex\", \"sign\", \"smokes\",\n",
    "                \"speaks\", \"status\"]\n",
    "    label_col = \"rating\"\n",
    "\n",
    "    # split data to 3 sets    \n",
    "    length = len(merged_df)\n",
    "    train_data = merged_df.loc[:0.8*length-1]\n",
    "    valid_data = merged_df.loc[0.8*length:0.9*length-1]\n",
    "    test_data = merged_df.loc[0.9*length:]\n",
    "\n",
    "    #Encoding categorical variables with the oerdinal encoder\n",
    "    ord_encoder = ce.ordinal.OrdinalEncoder(cols=cate_cols)\n",
    "    train_x, train_y = encode_csv(train_data, ord_encoder, label_col)\n",
    "    valid_x, valid_y = encode_csv(valid_data, ord_encoder, label_col, \"transform\")\n",
    "    test_x, test_y = encode_csv(test_data, ord_encoder, label_col, \"transform\")\n",
    "\n",
    "    lgb_train = lgb.Dataset(train_x, train_y.reshape(-1), params=params, categorical_feature=cate_cols)\n",
    "    lgb_valid = lgb.Dataset(valid_x, valid_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "    lgb_test = lgb.Dataset(test_x, test_y.reshape(-1), reference=lgb_train, categorical_feature=cate_cols)\n",
    "    lgb_model = lgb.train(params,\n",
    "                        lgb_train,\n",
    "                        num_boost_round=NUM_OF_TREES,\n",
    "                        valid_sets=lgb_valid,\n",
    "                        categorical_feature=cate_cols,\n",
    "                        callbacks=[lgb.early_stopping(EARLY_STOPPING_ROUNDS)])\n",
    "    \n",
    "    return lgb_model, ord_encoder\n",
    "\n",
    "def generate_lightGBM_recommendations(df: pd.DataFrame, lgb_model, ord_encoder, number_of_recommendations: int = 10) -> list:\n",
    "    label_col = \"rating\"\n",
    "    # Make predictions for all profiles\n",
    "    full_dataset_x, _ = encode_csv(df, ord_encoder, label_col, \"transform\")\n",
    "    all_preds = lgb_model.predict(full_dataset_x)\n",
    "\n",
    "    # Get sorted predictions with the highest one first\n",
    "    top_indices = np.argsort(all_preds)[::-1]\n",
    "\n",
    "    # Get the top recommendations\n",
    "    recommendations = []\n",
    "    counter = 0\n",
    "    for index in top_indices:\n",
    "        if counter == number_of_recommendations:\n",
    "            break\n",
    "        if not np.isnan(df.iloc[index][label_col]):\n",
    "            continue\n",
    "        else:\n",
    "            counter += 1\n",
    "            recommendations.append((index, all_preds[index]))\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 3557, number of negative: 44399\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011809 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 5176\n",
      "[LightGBM] [Info] Number of data points in the train set: 47956, number of used features: 21\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.074172 -> initscore=-2.524299\n",
      "[LightGBM] [Info] Start training from score -2.524299\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "Training until validation scores don't improve for 200 rounds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marianareyes/Library/Python/3.9/lib/python/site-packages/lightgbm/basic.py:1780: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/Users/marianareyes/Library/Python/3.9/lib/python/site-packages/lightgbm/basic.py:1513: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning(f'{cat_alias} in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Did not meet early stopping. Best iteration is:\n",
      "[55]\tvalid_0's auc: 0.978698\n",
      "Top 10 recommendations:\n",
      "Recommendation 1:\n",
      "Profile Index: 59008, Prediction: 0.9007063810574047\n",
      "Recommendation 2:\n",
      "Profile Index: 58642, Prediction: 0.8956106028457942\n",
      "Recommendation 3:\n",
      "Profile Index: 53144, Prediction: 0.8850699629720723\n",
      "Recommendation 4:\n",
      "Profile Index: 58340, Prediction: 0.8838723299329732\n",
      "Recommendation 5:\n",
      "Profile Index: 57179, Prediction: 0.8825521548313909\n",
      "Recommendation 6:\n",
      "Profile Index: 54206, Prediction: 0.881697215937644\n",
      "Recommendation 7:\n",
      "Profile Index: 50384, Prediction: 0.8791855945824912\n",
      "Recommendation 8:\n",
      "Profile Index: 48911, Prediction: 0.876613299043058\n",
      "Recommendation 9:\n",
      "Profile Index: 59264, Prediction: 0.8726835635823694\n",
      "Recommendation 10:\n",
      "Profile Index: 49035, Prediction: 0.8697389880014906\n"
     ]
    }
   ],
   "source": [
    "#TO be called in main\n",
    "path = \"profiles.csv\"\n",
    "\n",
    "df_preprocessed = preprocess_df(path)\n",
    "\n",
    "#Defining profile\n",
    "given_profile = {\n",
    "    'age': 26,\n",
    "    'body_type': 'curvy',\n",
    "    'diet': 'mostly anything',\n",
    "    'drinks': 'socially',\n",
    "    'drugs': 'never',\n",
    "    'education': 'working on college/university',\n",
    "    'ethnicity': 'hispanic / latin, white',\n",
    "    'height': 63.0,\n",
    "    'income': 20000,\n",
    "    'job': 'sales / marketing / biz dev',\n",
    "    'last_online': '2012-06-23-23-10',\n",
    "    'location': 'berkeley, california',\n",
    "    'offspring': 'doesn’t have kids, but might want them',\n",
    "    'orientation': 'gay',\n",
    "    'pets': 'likes dogs and likes cats',\n",
    "    'religion': 'catholicism and laughing about it',\n",
    "    'sex': 'f',\n",
    "    'sign': 'gemini and it’s fun to think about',\n",
    "    'smokes': 'no',\n",
    "    'speaks': 'english',\n",
    "    'status': 'single'\n",
    "}\n",
    "\n",
    "#Computing the ratings\n",
    "rated_df = ratings_prediction(given_profile, df_preprocessed)\n",
    "\n",
    "#Preprocessing dataset for lgbm\n",
    "preprocessed_lgbm = preprocess_lgbm(path, rated_df)\n",
    "\n",
    "#training lgbm\n",
    "lgb_model, ord_encoder = training_lgbm(preprocessed_lgbm)\n",
    "\n",
    "#Getting recommendations\n",
    "recommendations = generate_lightGBM_recommendations(preprocessed_lgbm, lgb_model, ord_encoder, 10)\n",
    "print(\"Top 10 recommendations:\")\n",
    "for index, (profile_index, prediction) in enumerate(recommendations, start=1):\n",
    "    print(f\"Recommendation {index}:\")\n",
    "    print(f\"Profile Index: {profile_index}, Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
